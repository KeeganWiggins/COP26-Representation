{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d310ab44",
   "metadata": {},
   "source": [
    "# Data Story : COP26 Statements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a9a3c4",
   "metadata": {},
   "source": [
    "#### The break down of the data story:\n",
    "\n",
    "For this assignment, I am analyzing statements made by representatives from countries at COP26. With this data, I want to see what the overall sentiment was at, and how often statements referenced the major goals of COP26. For example, I want to see how many times fossil fuels are mentioned (especially since this was a touchy subject at the end of COP26) from each category (Global North countries, Global South countries, and final report). Additionally, I want to see (using the bottom-up method) which themes/ topics are addresses more (i.e., energy). By looking for this, I think I'll be able to get a good idea of what the climate priorities are for each category.\n",
    "\n",
    "When I collected the data, I categorized the statements into two categories: Global North and Global South. The statements analyzed are only the ones that were in English and had an accessible pdf file to collect from this webiste: https://unfccc.int/cop26/speeches-and-statements and then the final report was collected here: https://www.bbc.com/news/science-environment-58982445\n",
    "\n",
    "For total transparency, I downloaded each individual statement and then combined them into larger pdf files based on the categories mentioned above. From there, I converted the three pdf files into csv files, using this website: https://cdkm.com/pdf-to-csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f658f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2c498e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np                  \n",
    "import pandas as pd                 \n",
    "import matplotlib.pyplot as plt     \n",
    "%matplotlib inline\n",
    "import seaborn as sns               \n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0160f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48b825a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import FreqDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6f2971",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6194785",
   "metadata": {},
   "source": [
    "## Analyzing Statements from the Global North"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c46a4f4",
   "metadata": {},
   "source": [
    "#### Cleaning Text\n",
    "This section takes statements made by representatives from the Global North and cleans the text by removing the stopwords and punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7770569b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('GN_Statements_EN.csv', header=None)\n",
    "#this is reading the csv file that contains statements from representatives in the global north"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720d9bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = ['Text']\n",
    "#for the aesthetic and naming the column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a62700e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc632e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "df['tokenized'] = df.apply(lambda row: nltk.word_tokenize(row['Text']), axis=1)\n",
    "\n",
    "#this code comes from Vlad and is the start of removing stopwords from the Global North csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ccbfeab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e27261",
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_stopwords = set(stopwords.words('english'))\n",
    "df['tokenized'] = df['tokenized'].apply(lambda x: [item for item in x if item not in remove_stopwords])\n",
    "\n",
    "#this code also comes from Vlad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9654e2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tokenized']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ed380c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tokenized'] = df['tokenized'].apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9663ea1e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['tokenized']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6555e4e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3afc5ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['GN_Text_Clean'] = df['tokenized'].str.replace(r\"[^a-zA-Z]+\", \" \").str.strip()\n",
    "#This regression came from Ramsha and is used to remove punctuation from df['tokenized']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0ede63",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['GN_Text_Clean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d7e141",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('GN_Statements_Clean.csv')\n",
    "#Got this idea from Claudia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6ac8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "GN = pd.read_csv('GN_Statements_Clean.csv', header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b148208f",
   "metadata": {},
   "outputs": [],
   "source": [
    "GN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98788e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "GN[\"GN_Text_Clean\"] = GN[\"GN_Text_Clean\"].astype(str)\n",
    "#Got this from Claudia, here I'm making the words in column GN_Text_Clean go from floaters to string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f6259e",
   "metadata": {},
   "outputs": [],
   "source": [
    "GN = pd.read_csv('GN_Statements_Clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb4f945",
   "metadata": {},
   "outputs": [],
   "source": [
    "GN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9400faae",
   "metadata": {},
   "outputs": [],
   "source": [
    "GN.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758cf9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "GN1 = GN.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67c0287",
   "metadata": {},
   "outputs": [],
   "source": [
    "GN1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3551f91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "GN1.drop(['Unnamed: 0'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b4df10",
   "metadata": {},
   "outputs": [],
   "source": [
    "GN1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faaac699",
   "metadata": {},
   "source": [
    "#### Themes\n",
    "This section takes statements made by representatives from the Global North to see what are the most common words mentioned to gage the major themes (i.e., energy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d8d209",
   "metadata": {},
   "outputs": [],
   "source": [
    "GN_Themes = Counter(\" \".join(GN1.GN_Text_Clean).split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f710a9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "GN_Themes.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239aa14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mywordcloud = WordCloud(background_color='white', width=800, height=400).generate(\" \".join(GN_Themes))\n",
    "plt.imshow(mywordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.title('Global North Statements')\n",
    "plt.savefig('GN.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47c187b",
   "metadata": {},
   "source": [
    "#### Frequency\n",
    "This section takes statements made by representatives from the Global North to see the frequency of words from the four main goals of COP26: a global net zero, protecting communities, mobilizing finance, and working together. The themes were picked based on this article: https://www.thenationalnews.com/world/uk-news/2021/07/07/key-themes-for-cop26-climate-summit-unveiled/\n",
    "\n",
    "In the following section, I got my code from http://www.learningaboutelectronics.com/Articles/How-to-find-the-number-of-times-a-word-or-phrase-occurs-in-a-text-in-Python-using-regular-expressions.php"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e79b324",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('GN_Statements_EN.csv' , 'r')\n",
    "read_data = file.read()\n",
    "per_word = read_data.split()\n",
    "print('Total Words: ', len(per_word))\n",
    "\n",
    "# Using this to know how many words are in document, which will become necessary for visualizing the fequency of \n",
    "# the specific words below, also used the original document for the original word count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122bff4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "phrase = list(GN1[\"GN_Text_Clean\"])\n",
    "makeitastring = ' '.join(map(str, phrase))\n",
    "#here I'm making the column GN_TexT_Clean from GN into a string for regular expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61fc5b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "makeitastring"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044a8ce0",
   "metadata": {},
   "source": [
    "### Global Net Zero\n",
    "First, looking at the goal of reaching a global net zero, I searched for terms (fossil fuels, emissions, renewables, and coal) to see how often the Global North statements fell into this category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91dc9171",
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns = [r'fossil fuels[^a-z,A-Z]+']\n",
    "\n",
    "for p in patterns:\n",
    "    match = re.findall(p, makeitastring)\n",
    "    print(match)\n",
    "    \n",
    "length= len(match)\n",
    "print(length)\n",
    "#using regular expression, I am able to see how often fossil fuels is mentioned\n",
    "#this was then used for other variations of fossil fuel, fossil, fuels and fuel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db41247",
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns = [r'fossil fuel[^a-z,A-Z]+']\n",
    "\n",
    "for p in patterns:\n",
    "    match = re.findall(p, makeitastring)\n",
    "    print(match)\n",
    "    \n",
    "length= len(match)\n",
    "print(length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510345a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns = [r'fossil[^a-z,A-Z]+']\n",
    "\n",
    "for p in patterns:\n",
    "    match = re.findall(p, makeitastring)\n",
    "    print(match)\n",
    "    \n",
    "length= len(match)\n",
    "print(length)\n",
    "\n",
    "# there was no result for 'fossils'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fea76eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns = [r'fuel[^a-z,A-Z]+']\n",
    "\n",
    "for p in patterns:\n",
    "    match = re.findall(p, makeitastring)\n",
    "    print(match)\n",
    "    \n",
    "length= len(match)\n",
    "print(length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be25072f",
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns = [r'fuels[^a-z,A-Z]+']\n",
    "\n",
    "for p in patterns:\n",
    "    match = re.findall(p, makeitastring)\n",
    "    print(match)\n",
    "    \n",
    "length= len(match)\n",
    "print(length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97fa41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns = [r'emissions[^a-z,A-Z]+']\n",
    "\n",
    "for p in patterns:\n",
    "    match = re.findall(p, makeitastring)\n",
    "    print(match)\n",
    "    \n",
    "length= len(match)\n",
    "print(length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c201dc85",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "patterns = [r'emission[^a-z,A-Z]+']\n",
    "\n",
    "for p in patterns:\n",
    "    match = re.findall(p, makeitastring)\n",
    "    print(match)\n",
    "    \n",
    "length= len(match)\n",
    "print(length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b8aeda",
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns = [r'renewables[^a-z,A-Z]+']\n",
    "\n",
    "for p in patterns:\n",
    "    match = re.findall(p, makeitastring)\n",
    "    print(match)\n",
    "    \n",
    "length= len(match)\n",
    "print(length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41082f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns = [r'renewable[^a-z,A-Z]+']\n",
    "\n",
    "for p in patterns:\n",
    "    match = re.findall(p, makeitastring)\n",
    "    print(match)\n",
    "    \n",
    "length= len(match)\n",
    "print(length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4781ea15",
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns = [r'coal[^a-z,A-Z]+']\n",
    "\n",
    "for p in patterns:\n",
    "    match = re.findall(p, makeitastring)\n",
    "    print(match)\n",
    "    \n",
    "length= len(match)\n",
    "print(length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0531161e",
   "metadata": {},
   "source": [
    "### Protecting Communities\n",
    "Next, looking at protecting communities, I searched for terms (protect and restore) to see how often the Global North statements fell into this category. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5b9474",
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns = [r'protect[^a-z,A-Z]+']\n",
    "\n",
    "for p in patterns:\n",
    "    match = re.findall(p, makeitastring)\n",
    "    print(match)\n",
    "    \n",
    "length= len(match)\n",
    "print(length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e3c73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns = [r'protecting[^a-z,A-Z]+']\n",
    "\n",
    "for p in patterns:\n",
    "    match = re.findall(p, makeitastring)\n",
    "    print(match)\n",
    "    \n",
    "length= len(match)\n",
    "print(length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2294e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns = [r'protects[^a-z,A-Z]+']\n",
    "\n",
    "for p in patterns:\n",
    "    match = re.findall(p, makeitastring)\n",
    "    print(match)\n",
    "    \n",
    "length= len(match)\n",
    "print(length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e4400d",
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns = [r'restore[^a-z,A-Z]+']\n",
    "\n",
    "for p in patterns:\n",
    "    match = re.findall(p, makeitastring)\n",
    "    print(match)\n",
    "    \n",
    "length= len(match)\n",
    "print(length)\n",
    "\n",
    "#also searched for restores, restored and restoring, but they were not found in the statements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f484e4e",
   "metadata": {},
   "source": [
    "### Mobilize Finance\n",
    "Next, looking into moblizing finance, I searched for the term \"finance\" to see how often the Global North statements fell into this category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676c9fda",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "patterns = [r'finance[^a-z,A-Z]+']\n",
    "\n",
    "for p in patterns:\n",
    "    match = re.findall(p, makeitastring)\n",
    "    print(match)\n",
    "    \n",
    "length= len(match)\n",
    "print(length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83419fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns = [r'finances[^a-z,A-Z]+']\n",
    "\n",
    "for p in patterns:\n",
    "    match = re.findall(p, makeitastring)\n",
    "    print(match)\n",
    "    \n",
    "length= len(match)\n",
    "print(length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c26e33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns = [r'financing[^a-z,A-Z]+']\n",
    "\n",
    "for p in patterns:\n",
    "    match = re.findall(p, makeitastring)\n",
    "    print(match)\n",
    "    \n",
    "length= len(match)\n",
    "print(length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa6f2708",
   "metadata": {},
   "source": [
    "### Working Together\n",
    "Lastly, looking into working together, I searched for terms (together & collaboration) to see how often the Global North statements fell into this category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5853284f",
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns = [r'together[^a-z,A-Z]+']\n",
    "\n",
    "for p in patterns:\n",
    "    match = re.findall(p, makeitastring)\n",
    "    print(match)\n",
    "    \n",
    "length= len(match)\n",
    "print(length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2626c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns = [r'collaboration[^a-z,A-Z]+']\n",
    "\n",
    "for p in patterns:\n",
    "    match = re.findall(p, makeitastring)\n",
    "    print(match)\n",
    "    \n",
    "length= len(match)\n",
    "print(length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f05e7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns = [r'collaborate[^a-z,A-Z]+']\n",
    "\n",
    "for p in patterns:\n",
    "    match = re.findall(p, makeitastring)\n",
    "    print(match)\n",
    "    \n",
    "length= len(match)\n",
    "print(length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dfbaa8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns = [r'collaborates[^a-z,A-Z]+']\n",
    "\n",
    "for p in patterns:\n",
    "    match = re.findall(p, makeitastring)\n",
    "    print(match)\n",
    "    \n",
    "length= len(match)\n",
    "print(length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef351c39",
   "metadata": {},
   "source": [
    "### Using Frequency Distrubition\n",
    "\n",
    "Here, I wanted to run a frequency distribution in order to plot the most frenquently used terms. I thought this could have been an interesting visualization for the different goals of COP26. However, I had problems with the code and could not figure out the issue without help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6a7774",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Statements = GN1['GN_Text_Clean']\n",
    "#Statements\n",
    "\n",
    "#words: list[str] = nltk.word_tokenize('Statements')\n",
    "#fd = nltk.FreqDist(words)\n",
    "#frequency distribution comes from this website: https://python.gotrained.com/frequency-distribution-in-nltk/\n",
    "\n",
    "#fdist = nltk.FreqDist('GN_Clean_Text')\n",
    "#freqDist = FreqDist('GN_Clean_Text')\n",
    "#print(freqDist)\n",
    "\n",
    "#words = freqDist.keys()\n",
    "#print(type(words))\n",
    "\n",
    "#words\n",
    "\n",
    "#print(len(words))\n",
    "\n",
    "#freDist.plot(11)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1596113f",
   "metadata": {},
   "source": [
    "#### Sentiment\n",
    "This section takes statements made by representatives from the Global North to analyze the sentiment behind references of fuels and fossil fuels. In the following section, I got my python code from https://towardsdatascience.com/a-beginners-guide-to-sentiment-analysis-in-python-95e354ea84f6 and https://realpython.com/python-nltk-sentiment-analysis/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fbf66d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "sia.polarity_scores(\"Wow, NLTK is really powerful!\")\n",
    "#code comes from Freddy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6a6cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "statements =GN1['Text'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92aac0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "\n",
    "def is_positive(statement: str) -> bool:\n",
    "    \"\"\"True if tweet has positive compound sentiment, False otherwise.\"\"\"\n",
    "    return sia.polarity_scores(statement)[\"compound\"] > 0\n",
    "\n",
    "shuffle(statements)\n",
    "for statement in statements[:10]:\n",
    "    print(\">\", is_positive(statement), statement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f5debd",
   "metadata": {},
   "outputs": [],
   "source": [
    "GN1['sentiment'] = GN1['GN_Text_Clean'].apply(lambda x: sia.polarity_scores(x)['compound'])\n",
    "#code comes from Vlad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbdeb595",
   "metadata": {},
   "outputs": [],
   "source": [
    "sia.polarity_scores(\"random text\")['compound']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe06425b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "GN1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7a9ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "GN1.to_csv('GN_Statements_Sentiment.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2086a71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "negative = GN1['sentiment'] <= -0.25\n",
    "neutral = (GN1['sentiment'] < 0.25) & (GN1['sentiment'] > -0.25)\n",
    "positive = GN1['sentiment'] >= 0.25\n",
    "#Code from Wei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ef0da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "GN1[neutral].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c347ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "GN1[positive].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9adedea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "GN1[negative].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da75c22",
   "metadata": {},
   "source": [
    "# Analyzing Statements From Global South"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70a8d2d",
   "metadata": {},
   "source": [
    "#### Cleaning Text\n",
    "This section takes statements made by representatives from the Global North and cleans the text by removing the stopwords and punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3997f193",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('GS_Statements_EN.csv', header=None)\n",
    "#this is reading the csv file that contains statements from representatives in the global south"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077f7e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = ['Text']\n",
    "#for the aesthetic and naming the column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d70aa5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa33f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "df['tokenized'] = df.apply(lambda row: nltk.word_tokenize(row['Text']), axis=1)\n",
    "\n",
    "#this code comes from Vlad and is the start of removing stopwords from the Global South csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab6aa4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909f1e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_stopwords = set(stopwords.words('english'))\n",
    "df['tokenized'] = df['tokenized'].apply(lambda x: [item for item in x if item not in remove_stopwords])\n",
    "\n",
    "#this code also comes from Vlad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7481d517",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tokenized']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab33e4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tokenized'] = df['tokenized'].apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34314692",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tokenized']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d93c293",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18179486",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['GS_Text_Clean'] = df['tokenized'].str.replace(r\"[^a-zA-Z]+\", \" \").str.strip()\n",
    "#This regression came from Ramsha and is used to remove punctuation from df['tokenized']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4779442",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['GS_Text_Clean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d1068b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('GS_Statements_Clean.csv')\n",
    "#Got this idea from Claudia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0784f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "GS = pd.read_csv('GS_Statements_Clean.csv', header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9793eee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "GS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4596d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "GS[\"GS_Text_Clean\"] = GS[\"GS_Text_Clean\"].astype(str)\n",
    "#Got this from Claudia, here I'm making the words in column GN_Text_Clean go from floaters to string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c766e6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "GS = pd.read_csv('GS_Statements_Clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530178af",
   "metadata": {},
   "outputs": [],
   "source": [
    "GS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f01464",
   "metadata": {},
   "outputs": [],
   "source": [
    "GS.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30fddc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "GS1 = GS.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1997706e",
   "metadata": {},
   "outputs": [],
   "source": [
    "GS1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4a1662",
   "metadata": {},
   "outputs": [],
   "source": [
    "GS1.drop(['Unnamed: 0'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bdf3f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "GS1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e679447d",
   "metadata": {},
   "source": [
    "#### Themes\n",
    "This section takes statements made by representatives from the Global North to see what are the most common words mentioned to gage the major themes (i.e., energy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1a7120",
   "metadata": {},
   "outputs": [],
   "source": [
    "GS_Themes = Counter(\" \".join(GS1.GS_Text_Clean).split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f726b5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "GS_Themes.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6c58c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "mywordcloud = WordCloud(background_color='white', width=800, height=400).generate(\" \".join(GS_Themes))\n",
    "plt.imshow(mywordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.title('Global South Statements')\n",
    "plt.savefig('GS.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2cd723",
   "metadata": {},
   "source": [
    "#### Frequency\n",
    "This section takes statements made by representatives from the Global South to see the frequency of words from the four main goals of COP26: a global net zero, protecting communities, mobilizing finance, and working together. The themes were picked based on this article: https://www.thenationalnews.com/world/uk-news/2021/07/07/key-themes-for-cop26-climate-summit-unveiled/ \n",
    "In the following section, I got my code from http://www.learningaboutelectronics.com/Articles/How-to-find-the-number-of-times-a-word-or-phrase-occurs-in-a-text-in-Python-using-regular-expressions.php"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f73e2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('GS_Statements_EN.csv' , 'r')\n",
    "read_data = file.read()\n",
    "per_word = read_data.split()\n",
    "print('Total Words: ', len(per_word))\n",
    "\n",
    "# Using this to know how many words are in document, which will become necessary for visualizing the fequency of \n",
    "# the specific words below, also used the original document for the original word count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49fd173",
   "metadata": {},
   "outputs": [],
   "source": [
    "phrase = list(GS1[\"GS_Text_Clean\"])\n",
    "makeitastring = ' '.join(map(str, phrase))\n",
    "#here I'm making the column GS_TexT_Clean from GS into a string for regular expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24c8d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "makeitastring"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e16458",
   "metadata": {},
   "source": [
    "### Global Net Zero\n",
    "First, looking at the goal of reaching a global net zero, I searched for terms (fossil fuels, emissions, renewables, and coal) to see how often the Global South statements fell into this category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c02328",
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns = [r'fossil fuels[^a-z,A-Z]+']\n",
    "\n",
    "for p in patterns:\n",
    "    match = re.findall(p, makeitastring)\n",
    "    print(match)\n",
    "    \n",
    "length= len(match)\n",
    "print(length)\n",
    "#using regular expression, I am able to see how often fossil fuels is mentioned\n",
    "#this was then used for other variations of fossil fuel, fossil, fuels and fuel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ded6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns = [r'fossil fuel[^a-z,A-Z]+']\n",
    "\n",
    "for p in patterns:\n",
    "    match = re.findall(p, makeitastring)\n",
    "    print(match)\n",
    "    \n",
    "length= len(match)\n",
    "print(length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f858dd96",
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns = [r'fossil[^a-z,A-Z]+']\n",
    "\n",
    "for p in patterns:\n",
    "    match = re.findall(p, makeitastring)\n",
    "    print(match)\n",
    "    \n",
    "length= len(match)\n",
    "print(length)\n",
    "\n",
    "#there were no results for fossils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e5d6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns = [r'fuel[^a-z,A-Z]+']\n",
    "\n",
    "for p in patterns:\n",
    "    match = re.findall(p, makeitastring)\n",
    "    print(match)\n",
    "    \n",
    "length= len(match)\n",
    "print(length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0df6cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns = [r'fuels[^a-z,A-Z]+']\n",
    "\n",
    "for p in patterns:\n",
    "    match = re.findall(p, makeitastring)\n",
    "    print(match)\n",
    "    \n",
    "length= len(match)\n",
    "print(length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e813d54",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "patterns = [r'emissions[^a-z,A-Z]+']\n",
    "\n",
    "for p in patterns:\n",
    "    match = re.findall(p, makeitastring)\n",
    "    print(match)\n",
    "    \n",
    "length= len(match)\n",
    "print(length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2bb745",
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns = [r'emission[^a-z,A-Z]+']\n",
    "\n",
    "for p in patterns:\n",
    "    match = re.findall(p, makeitastring)\n",
    "    print(match)\n",
    "    \n",
    "length= len(match)\n",
    "print(length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0476147d",
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns = [r'renewables[^a-z,A-Z]+']\n",
    "\n",
    "for p in patterns:\n",
    "    match = re.findall(p, makeitastring)\n",
    "    print(match)\n",
    "    \n",
    "length= len(match)\n",
    "print(length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef36e15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns = [r'renewable[^a-z,A-Z]+']\n",
    "\n",
    "for p in patterns:\n",
    "    match = re.findall(p, makeitastring)\n",
    "    print(match)\n",
    "    \n",
    "length= len(match)\n",
    "print(length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf64b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns = [r'coal[^a-z,A-Z]+']\n",
    "\n",
    "for p in patterns:\n",
    "    match = re.findall(p, makeitastring)\n",
    "    print(match)\n",
    "    \n",
    "length= len(match)\n",
    "print(length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c463bcf",
   "metadata": {},
   "source": [
    "### Protecting Communities\n",
    "Next, looking at protecting communities, I searched for terms (protect and restore) to see how often the Global South statements fell into this category. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4621b9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns = [r'protect[^a-z,A-Z]+']\n",
    "\n",
    "for p in patterns:\n",
    "    match = re.findall(p, makeitastring)\n",
    "    print(match)\n",
    "    \n",
    "length= len(match)\n",
    "print(length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe9ac70",
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns = [r'protecting[^a-z,A-Z]+']\n",
    "\n",
    "for p in patterns:\n",
    "    match = re.findall(p, makeitastring)\n",
    "    print(match)\n",
    "    \n",
    "length= len(match)\n",
    "print(length)\n",
    "\n",
    "#there were no results for 'protects'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98341575",
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns = [r'restore[^a-z,A-Z]+']\n",
    "\n",
    "for p in patterns:\n",
    "    match = re.findall(p, makeitastring)\n",
    "    print(match)\n",
    "    \n",
    "length= len(match)\n",
    "print(length)\n",
    "\n",
    "#there were no results for 'restores' or 'restoring'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3136981f",
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns = [r'restored[^a-z,A-Z]+']\n",
    "\n",
    "for p in patterns:\n",
    "    match = re.findall(p, makeitastring)\n",
    "    print(match)\n",
    "    \n",
    "length= len(match)\n",
    "print(length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324140a9",
   "metadata": {},
   "source": [
    "### Mobilize Finance\n",
    "Next, looking into moblizing finance, I searched for the term \"finance\" to see how often the Global South statements fell into this category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024e950b",
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns = [r'finance[^a-z,A-Z]+']\n",
    "\n",
    "for p in patterns:\n",
    "    match = re.findall(p, makeitastring)\n",
    "    print(match)\n",
    "    \n",
    "length= len(match)\n",
    "print(length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff581669",
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns = [r'finances[^a-z,A-Z]+']\n",
    "\n",
    "for p in patterns:\n",
    "    match = re.findall(p, makeitastring)\n",
    "    print(match)\n",
    "    \n",
    "length= len(match)\n",
    "print(length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1552f8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns = [r'financing[^a-z,A-Z]+']\n",
    "\n",
    "for p in patterns:\n",
    "    match = re.findall(p, makeitastring)\n",
    "    print(match)\n",
    "    \n",
    "length= len(match)\n",
    "print(length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f23a3ac6",
   "metadata": {},
   "source": [
    "### Working Together\n",
    "Lastly, looking into working together, I searched for terms (together & collaboration) to see how often the Global South statements fell into this category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2c0036",
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns = [r'together[^a-z,A-Z]+']\n",
    "\n",
    "for p in patterns:\n",
    "    match = re.findall(p, makeitastring)\n",
    "    print(match)\n",
    "    \n",
    "length= len(match)\n",
    "print(length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e556b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns = [r'collaboration[^a-z,A-Z]+']\n",
    "\n",
    "for p in patterns:\n",
    "    match = re.findall(p, makeitastring)\n",
    "    print(match)\n",
    "    \n",
    "length= len(match)\n",
    "print(length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013160bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns = [r'collaborate[^a-z,A-Z]+']\n",
    "\n",
    "for p in patterns:\n",
    "    match = re.findall(p, makeitastring)\n",
    "    print(match)\n",
    "    \n",
    "length= len(match)\n",
    "print(length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b408ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns = [r'collaborates[^a-z,A-Z]+']\n",
    "\n",
    "for p in patterns:\n",
    "    match = re.findall(p, makeitastring)\n",
    "    print(match)\n",
    "    \n",
    "length= len(match)\n",
    "print(length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1150a773",
   "metadata": {},
   "source": [
    "#### Sentiment\n",
    "This section takes statements made by representatives from the Global South to analyze the sentiment behind references of fuels and fossil fuels. In the following section, I got my python code from https://towardsdatascience.com/a-beginners-guide-to-sentiment-analysis-in-python-95e354ea84f6 and https://realpython.com/python-nltk-sentiment-analysis/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe05504e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "sia.polarity_scores(\"Wow, NLTK is really powerful!\")\n",
    "#code comes from Freddy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfdcdbd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "statements =GS1['Text'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb96a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "\n",
    "def is_positive(statement: str) -> bool:\n",
    "    \"\"\"True if tweet has positive compound sentiment, False otherwise.\"\"\"\n",
    "    return sia.polarity_scores(statement)[\"compound\"] > 0\n",
    "\n",
    "shuffle(statements)\n",
    "for statement in statements[:10]:\n",
    "    print(\">\", is_positive(statement), statement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6782b654",
   "metadata": {},
   "outputs": [],
   "source": [
    "GS1['sentiment'] = GS1['GS_Text_Clean'].apply(lambda x: sia.polarity_scores(x)['compound'])\n",
    "#code comes from Vlad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39bf198d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sia.polarity_scores(\"random text\")['compound']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c268e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "GS1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13fd8466",
   "metadata": {},
   "outputs": [],
   "source": [
    "GS1.to_csv('GS_Statements_Sentiment.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e13b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "negative = GS1['sentiment'] <= -0.25\n",
    "neutral = (GS1['sentiment'] < 0.25) & (GS1['sentiment'] > -0.25)\n",
    "positive = GS1['sentiment'] >= 0.25\n",
    "#Code from Wei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3d5cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "GS1[neutral].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52a1262",
   "metadata": {},
   "outputs": [],
   "source": [
    "GS1[positive].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b18f0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "GS1[negative].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ea644c",
   "metadata": {},
   "source": [
    "# Analyzing the Final Report from COP26"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f34b00",
   "metadata": {},
   "source": [
    "#### Cleaning Text\n",
    "This section takes statements made by representatives from the Global North and cleans the text by removing the stopwords and punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8faeae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('COP26_Final_Report.csv', header=None)\n",
    "#this is reading the csv file that contains the final report from COP26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ae6e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = ['Text']\n",
    "#for the aesthetic and naming the column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99654342",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8831db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "df['tokenized'] = df.apply(lambda row: nltk.word_tokenize(row['Text']), axis=1)\n",
    "\n",
    "#this code comes from Vlad and is the start of removing stopwords from the final report csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23fce37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1735246e",
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_stopwords = set(stopwords.words('english'))\n",
    "df['tokenized'] = df['tokenized'].apply(lambda x: [item for item in x if item not in remove_stopwords])\n",
    "\n",
    "#this code also comes from Vlad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a9e1f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tokenized']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a44a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tokenized'] = df['tokenized'].apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7409d67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tokenized']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368c6dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc53dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['FR_Text_Clean'] = df['tokenized'].str.replace(r\"[^a-zA-Z]+\", \" \").str.strip()\n",
    "#This regression came from Ramsha and is used to remove punctuation from df['tokenized']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ab690e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['FR_Text_Clean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee52ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('FR_Statements_Clean.csv')\n",
    "#Got this idea from Claudia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79432874",
   "metadata": {},
   "outputs": [],
   "source": [
    "FR = pd.read_csv('FR_Statements_Clean.csv', header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d43c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "FR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52251c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "FR[\"FR_Text_Clean\"] = FR[\"FR_Text_Clean\"].astype(str)\n",
    "#Got this from Claudia, here I'm making the words in column GN_Text_Clean go from floaters to string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176733f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "FR = pd.read_csv('FR_Statements_Clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b380504",
   "metadata": {},
   "outputs": [],
   "source": [
    "FR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c102bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "FR.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7539ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "FR1 = FR.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4d5abe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "FR1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0749cb20",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "FR1.drop(['Unnamed: 0'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3b26ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "FR1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614758ad",
   "metadata": {},
   "source": [
    "#### Themes\n",
    "This section takes statements made by representatives from the Global North to see what are the most common words mentioned to gage the major themes (i.e., energy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ca6587",
   "metadata": {},
   "outputs": [],
   "source": [
    "FR_Themes = Counter(\" \".join(FR1.FR_Text_Clean).split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f8a4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "FR_Themes.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26230023",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mywordcloud = WordCloud(background_color='white', width=800, height=400).generate(\" \".join(FR_Themes))\n",
    "plt.imshow(mywordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.title('Final Report')\n",
    "plt.savefig('FR.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2030b795",
   "metadata": {},
   "source": [
    "#### Frequency\n",
    "This section takes statements made by representatives from the final report to see the frequency of words from the four main goals of COP26: a global net zero, protecting communities, mobilizing finance, and working together. The themes were picked based on this article: https://www.thenationalnews.com/world/uk-news/2021/07/07/key-themes-for-cop26-climate-summit-unveiled/\n",
    "\n",
    "In the following section, I got my code from http://www.learningaboutelectronics.com/Articles/How-to-find-the-number-of-times-a-word-or-phrase-occurs-in-a-text-in-Python-using-regular-expressions.php"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d62fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('COP26_Final_Report.csv' , 'r')\n",
    "read_data = file.read()\n",
    "per_word = read_data.split()\n",
    "print('Total Words: ', len(per_word))\n",
    "\n",
    "# Using this to know how many words are in document, which will become necessary for visualizing the fequency of \n",
    "# the specific words below, also used the original document for the original word count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67317cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "phrase = list(FR1[\"FR_Text_Clean\"])\n",
    "makeitastring = ' '.join(map(str, phrase))\n",
    "#here I'm making the column GS_TexT_Clean from GS into a string for regular expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebbb3078",
   "metadata": {},
   "outputs": [],
   "source": [
    "makeitastring"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59bd01df",
   "metadata": {},
   "source": [
    "### Global Net Zero\n",
    "First, looking at the goal of reaching a global net zero, I searched for terms (fossil fuels, emissions, renewables, and coal) to see how often the final report falls into this category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e580aa7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns = [r'fossil fuels[^a-z,A-Z]+']\n",
    "\n",
    "for p in patterns:\n",
    "    match = re.findall(p, makeitastring)\n",
    "    print(match)\n",
    "    \n",
    "length= len(match)\n",
    "print(length)\n",
    "#using regular expression, I am able to see how often fossil fuels is mentioned\n",
    "#this was then used for other variations of fossil fuel, fossil, fuels and fuel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fcd9247",
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns = [r'fossil fuel[^a-z,A-Z]+']\n",
    "\n",
    "for p in patterns:\n",
    "    match = re.findall(p, makeitastring)\n",
    "    print(match)\n",
    "    \n",
    "length= len(match)\n",
    "print(length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5864ee10",
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns = [r'fossil[^a-z,A-Z]+']\n",
    "\n",
    "for p in patterns:\n",
    "    match = re.findall(p, makeitastring)\n",
    "    print(match)\n",
    "    \n",
    "length= len(match)\n",
    "print(length)\n",
    "\n",
    "# there was no result for fossils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b94b929",
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns = [r'fuel[^a-z,A-Z]+']\n",
    "\n",
    "for p in patterns:\n",
    "    match = re.findall(p, makeitastring)\n",
    "    print(match)\n",
    "    \n",
    "length= len(match)\n",
    "print(length)\n",
    "\n",
    "#There was no result for fuels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05451c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns = [r'emissions[^a-z,A-Z]+']\n",
    "\n",
    "for p in patterns:\n",
    "    match = re.findall(p, makeitastring)\n",
    "    print(match)\n",
    "    \n",
    "length= len(match)\n",
    "print(length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e06643",
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns = [r'emission[^a-z,A-Z]+']\n",
    "\n",
    "for p in patterns:\n",
    "    match = re.findall(p, makeitastring)\n",
    "    print(match)\n",
    "    \n",
    "length= len(match)\n",
    "print(length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6334be52",
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns = [r'coal[^a-z,A-Z]+']\n",
    "\n",
    "for p in patterns:\n",
    "    match = re.findall(p, makeitastring)\n",
    "    print(match)\n",
    "    \n",
    "length= len(match)\n",
    "print(length)\n",
    "\n",
    "# there were no results for 'renewables' or 'renewable'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59138c5f",
   "metadata": {},
   "source": [
    "### Protecting Communities\n",
    "Next, looking at protecting communities, I searched for terms (protect and restore) to see how often the final report falls into this category. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f17a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns = [r'protecting[^a-z,A-Z]+']\n",
    "\n",
    "for p in patterns:\n",
    "    match = re.findall(p, makeitastring)\n",
    "    print(match)\n",
    "    \n",
    "length= len(match)\n",
    "print(length)\n",
    "\n",
    "# there were no results for 'protect' or 'protects'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a03695",
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns = [r'restoring[^a-z,A-Z]+']\n",
    "\n",
    "for p in patterns:\n",
    "    match = re.findall(p, makeitastring)\n",
    "    print(match)\n",
    "    \n",
    "length= len(match)\n",
    "print(length)\n",
    "\n",
    "# there were no results for 'restore', 'restores' or 'restored'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22dcf018",
   "metadata": {},
   "source": [
    "### Mobilize Finance\n",
    "Next, looking into moblizing finance, I searched for the term \"finance\" to see how often the Global North statements fell into this category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6d17f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns = [r'finance[^a-z,A-Z]+']\n",
    "\n",
    "for p in patterns:\n",
    "    match = re.findall(p, makeitastring)\n",
    "    print(match)\n",
    "    \n",
    "length= len(match)\n",
    "print(length)\n",
    "\n",
    "# there were no results for 'finances' or 'financing'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b44ebe",
   "metadata": {},
   "source": [
    "### Working Together\n",
    "Lastly, looking into working together, I searched for terms (together & collaboration) to see how often the Global North statements fell into this category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5889c63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns = [r'together[^a-z,A-Z]+']\n",
    "\n",
    "for p in patterns:\n",
    "    match = re.findall(p, makeitastring)\n",
    "    print(match)\n",
    "    \n",
    "length= len(match)\n",
    "print(length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1f9dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns = [r'collaboration[^a-z,A-Z]+']\n",
    "\n",
    "for p in patterns:\n",
    "    match = re.findall(p, makeitastring)\n",
    "    print(match)\n",
    "    \n",
    "length= len(match)\n",
    "print(length)\n",
    "\n",
    "# there were no results for 'collaborate' or 'collaborates'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b46b6e",
   "metadata": {},
   "source": [
    "#### Sentiment\n",
    "This section takes statements made by representatives from the Global South to analyze the sentiment behind references of fuels and fossil fuels. In the following section, I got my python code from https://towardsdatascience.com/a-beginners-guide-to-sentiment-analysis-in-python-95e354ea84f6 and https://realpython.com/python-nltk-sentiment-analysis/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0dd356",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "sia.polarity_scores(\"Wow, NLTK is really powerful!\")\n",
    "#code comes from Freddy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b473fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "statements =FR1['Text'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67161e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "\n",
    "def is_positive(statement: str) -> bool:\n",
    "    \"\"\"True if tweet has positive compound sentiment, False otherwise.\"\"\"\n",
    "    return sia.polarity_scores(statement)[\"compound\"] > 0\n",
    "\n",
    "shuffle(statements)\n",
    "for statement in statements[:10]:\n",
    "    print(\">\", is_positive(statement), statement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e928c44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "FR1['sentiment'] = FR1['FR_Text_Clean'].apply(lambda x: sia.polarity_scores(x)['compound'])\n",
    "#code comes from Vlad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70c890b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sia.polarity_scores(\"random text\")['compound']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb673db",
   "metadata": {},
   "outputs": [],
   "source": [
    "FR1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f23bb65",
   "metadata": {},
   "outputs": [],
   "source": [
    "FR1.to_csv('FR_Statements_Sentiment.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee95087b",
   "metadata": {},
   "outputs": [],
   "source": [
    "negative = FR1['sentiment'] <= -0.25\n",
    "neutral = (FR1['sentiment'] < 0.25) & (FR1['sentiment'] > -0.25)\n",
    "positive = FR1['sentiment'] >= 0.25\n",
    "#Code from Wei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e93e32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "FR1[neutral].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db682976",
   "metadata": {},
   "outputs": [],
   "source": [
    "FR1[positive].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ec1f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "FR1[negative].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a6cac48",
   "metadata": {},
   "source": [
    "# Visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5fb3e3",
   "metadata": {},
   "source": [
    "The visualizations I've decided to include in my story include the wordcloud for each category: Global North, Global South, and the Final Report. Then I created two visualizations using datawrapper because it looked nicer and I was able to compare the different datasets. First, I used pie charts to demonstrate the different percentages of negative, positive or neutral a document was. Second, I used a clustered bar chart to demonstrate the amount of references to one of the four goals made."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
